{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yiting Wang 4646909520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "directory = '../data/Book Files/books/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(directory):\n",
    "    corpus = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            with open(root + f, encoding='ascii', errors='ignore') as book:\n",
    "                book_corpus = book.read().lower()\n",
    "                corpus.append(book_corpus)\n",
    "            print('Text: {}, String length: {}'.format(f, len(corpus[-1])))\n",
    "        corpus = sorted(corpus, key=lambda x : len(x))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: AIIMAT.txt, String length: 746219\n",
      "Text: MLOE.txt, String length: 412226\n",
      "Text: OKEWFSMP.txt, String length: 405741\n",
      "Text: TAM.txt, String length: 514652\n",
      "Text: TAMatter.txt, String length: 766542\n",
      "Text: THWP.txt, String length: 2005566\n",
      "Text: TPP.txt, String length: 244306\n",
      "[244306, 405741, 412226, 514652, 746219, 766542, 2005566]\n"
     ]
    }
   ],
   "source": [
    "Russell_corpus = create_corpus(directory)\n",
    "print([len(book) for book in Russell_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = []\n",
    "for n in Russell_corpus:\n",
    "    chars.extend(list(set(set(n))))\n",
    "\n",
    "chars = sorted(list(set(chars)))\n",
    "char_dict = dict((c, i) for i, c in enumerate(chars))\n",
    "int_dict = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c(iii), c(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_Corpus(corpus, win_size):\n",
    "    input_list, output_list = [], []\n",
    "    for w in range(0, len(corpus) - win_size + 1, 1):\n",
    "        seq_in = corpus[w :w + win_size - 1]\n",
    "        seq_out = corpus[w + win_size - 1]\n",
    "        input_list.append([char_dict[c] for c in seq_in])\n",
    "        output_list.append(char_dict[seq_out])\n",
    "    return input_list, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 100\n",
    "\n",
    "for n in Russell_corpus:\n",
    "    input_list, output_list = [], []\n",
    "    temp_in, temp_out = window_Corpus(n, win_size)\n",
    "    input_list.extend(temp_in)\n",
    "    output_list.extend(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2005467, 99, 1)\n"
     ]
    }
   ],
   "source": [
    "input_data = np.reshape(input_list, (len(input_list), win_size - 1, 1))\n",
    "print(input_data.shape)\n",
    "\n",
    "#Normalize data\n",
    "input_data = input_data / float(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2005467, 68)\n"
     ]
    }
   ],
   "source": [
    "#One hot spot\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "output_data = np_utils.to_categorical(output_list)\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c(vi), c(vii), c(viii), c(ix), c(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               264192    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 68)                17476     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 281,668\n",
      "Trainable params: 281,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Define the LSTM model\n",
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(256, input_shape = (input_data.shape[1], input_data.shape[2])))\n",
    "LSTM_model.add(Dropout(0.2))\n",
    "LSTM_model.add(Dense(output_data.shape[1], activation='softmax'))\n",
    "print(LSTM_model.summary())\n",
    "\n",
    "LSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "filepath = \"../LSTM/weights-improvement-{epoch:02d}-{loss:.2f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 2.1199\n",
      "Epoch 00001: loss improved from 2.22162 to 2.11993, saving model to ../LSTM\\weights-improvement-01-2.12-bigger.hdf5\n",
      "15668/15668 [==============================] - 4062s 259ms/step - loss: 2.1199\n",
      "Epoch 2/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 2.0526\n",
      "Epoch 00002: loss improved from 2.11993 to 2.05258, saving model to ../LSTM\\weights-improvement-02-2.05-bigger.hdf5\n",
      "15668/15668 [==============================] - 4211s 269ms/step - loss: 2.0526\n",
      "Epoch 3/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 2.0013\n",
      "Epoch 00003: loss improved from 2.05258 to 2.00130, saving model to ../LSTM\\weights-improvement-03-2.00-bigger.hdf5\n",
      "15668/15668 [==============================] - 4282s 273ms/step - loss: 2.0013\n",
      "Epoch 4/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.9614\n",
      "Epoch 00004: loss improved from 2.00130 to 1.96144, saving model to ../LSTM\\weights-improvement-04-1.96-bigger.hdf5\n",
      "15668/15668 [==============================] - 4115s 263ms/step - loss: 1.9614\n",
      "Epoch 5/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.9295\n",
      "Epoch 00005: loss improved from 1.96144 to 1.92951, saving model to ../LSTM\\weights-improvement-05-1.93-bigger.hdf5\n",
      "15668/15668 [==============================] - 4076s 260ms/step - loss: 1.9295\n",
      "Epoch 6/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.9038\n",
      "Epoch 00006: loss improved from 1.92951 to 1.90381, saving model to ../LSTM\\weights-improvement-06-1.90-bigger.hdf5\n",
      "15668/15668 [==============================] - 4189s 267ms/step - loss: 1.9038\n",
      "Epoch 7/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.8824\n",
      "Epoch 00007: loss improved from 1.90381 to 1.88238, saving model to ../LSTM\\weights-improvement-07-1.88-bigger.hdf5\n",
      "15668/15668 [==============================] - 4050s 258ms/step - loss: 1.8824\n",
      "Epoch 8/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.8632\n",
      "Epoch 00008: loss improved from 1.88238 to 1.86324, saving model to ../LSTM\\weights-improvement-08-1.86-bigger.hdf5\n",
      "15668/15668 [==============================] - 4048s 258ms/step - loss: 1.8632\n",
      "Epoch 9/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.8458\n",
      "Epoch 00009: loss improved from 1.86324 to 1.84585, saving model to ../LSTM\\weights-improvement-09-1.85-bigger.hdf5\n",
      "15668/15668 [==============================] - 4069s 260ms/step - loss: 1.8458\n",
      "Epoch 10/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.8311\n",
      "Epoch 00010: loss improved from 1.84585 to 1.83114, saving model to ../LSTM\\weights-improvement-10-1.83-bigger.hdf5\n",
      "15668/15668 [==============================] - 4117s 263ms/step - loss: 1.8311\n",
      "Epoch 11/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.8188\n",
      "Epoch 00011: loss improved from 1.83114 to 1.81880, saving model to ../LSTM\\weights-improvement-11-1.82-bigger.hdf5\n",
      "15668/15668 [==============================] - 4536s 290ms/step - loss: 1.8188\n",
      "Epoch 12/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.8068\n",
      "Epoch 00012: loss improved from 1.81880 to 1.80684, saving model to ../LSTM\\weights-improvement-12-1.81-bigger.hdf5\n",
      "15668/15668 [==============================] - 4611s 294ms/step - loss: 1.8068\n",
      "Epoch 13/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7961\n",
      "Epoch 00013: loss improved from 1.80684 to 1.79611, saving model to ../LSTM\\weights-improvement-13-1.80-bigger.hdf5\n",
      "15668/15668 [==============================] - 4104s 262ms/step - loss: 1.7961\n",
      "Epoch 14/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7866\n",
      "Epoch 00014: loss improved from 1.79611 to 1.78662, saving model to ../LSTM\\weights-improvement-14-1.79-bigger.hdf5\n",
      "15668/15668 [==============================] - 4056s 259ms/step - loss: 1.7866\n",
      "Epoch 15/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7773\n",
      "Epoch 00015: loss improved from 1.78662 to 1.77734, saving model to ../LSTM\\weights-improvement-15-1.78-bigger.hdf5\n",
      "15668/15668 [==============================] - 4027s 257ms/step - loss: 1.7773\n",
      "Epoch 16/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7694\n",
      "Epoch 00016: loss improved from 1.77734 to 1.76937, saving model to ../LSTM\\weights-improvement-16-1.77-bigger.hdf5\n",
      "15668/15668 [==============================] - 3970s 253ms/step - loss: 1.7694\n",
      "Epoch 17/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7834\n",
      "Epoch 00017: loss did not improve from 1.76937\n",
      "15668/15668 [==============================] - 3937s 251ms/step - loss: 1.7834\n",
      "Epoch 18/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7567\n",
      "Epoch 00018: loss improved from 1.76937 to 1.75675, saving model to ../LSTM\\weights-improvement-18-1.76-bigger.hdf5\n",
      "15668/15668 [==============================] - 3902s 249ms/step - loss: 1.7567\n",
      "Epoch 19/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7495\n",
      "Epoch 00019: loss improved from 1.75675 to 1.74952, saving model to ../LSTM\\weights-improvement-19-1.75-bigger.hdf5\n",
      "15668/15668 [==============================] - 3867s 247ms/step - loss: 1.7495\n",
      "Epoch 20/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7418\n",
      "Epoch 00020: loss improved from 1.74952 to 1.74183, saving model to ../LSTM\\weights-improvement-20-1.74-bigger.hdf5\n",
      "15668/15668 [==============================] - 3850s 246ms/step - loss: 1.7418\n",
      "Epoch 21/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7363\n",
      "Epoch 00021: loss improved from 1.74183 to 1.73626, saving model to ../LSTM\\weights-improvement-21-1.74-bigger.hdf5\n",
      "15668/15668 [==============================] - 3845s 245ms/step - loss: 1.7363\n",
      "Epoch 22/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7296\n",
      "Epoch 00022: loss improved from 1.73626 to 1.72960, saving model to ../LSTM\\weights-improvement-22-1.73-bigger.hdf5\n",
      "15668/15668 [==============================] - 3890s 248ms/step - loss: 1.7296\n",
      "Epoch 23/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7252\n",
      "Epoch 00023: loss improved from 1.72960 to 1.72523, saving model to ../LSTM\\weights-improvement-23-1.73-bigger.hdf5\n",
      "15668/15668 [==============================] - 4032s 257ms/step - loss: 1.7252\n",
      "Epoch 24/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7188\n",
      "Epoch 00024: loss improved from 1.72523 to 1.71879, saving model to ../LSTM\\weights-improvement-24-1.72-bigger.hdf5\n",
      "15668/15668 [==============================] - 4113s 263ms/step - loss: 1.7188\n",
      "Epoch 25/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7137\n",
      "Epoch 00025: loss improved from 1.71879 to 1.71373, saving model to ../LSTM\\weights-improvement-25-1.71-bigger.hdf5\n",
      "15668/15668 [==============================] - 4078s 260ms/step - loss: 1.7137\n",
      "Epoch 26/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7095\n",
      "Epoch 00026: loss improved from 1.71373 to 1.70946, saving model to ../LSTM\\weights-improvement-26-1.71-bigger.hdf5\n",
      "15668/15668 [==============================] - 4094s 261ms/step - loss: 1.7095\n",
      "Epoch 27/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7048\n",
      "Epoch 00027: loss improved from 1.70946 to 1.70483, saving model to ../LSTM\\weights-improvement-27-1.70-bigger.hdf5\n",
      "15668/15668 [==============================] - 4085s 261ms/step - loss: 1.7048\n",
      "Epoch 28/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 1.7093\n",
      "Epoch 00028: loss did not improve from 1.70483\n",
      "15668/15668 [==============================] - 4026s 257ms/step - loss: 1.7093\n",
      "Epoch 29/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 2.5662\n",
      "Epoch 00029: loss did not improve from 1.70483\n",
      "15668/15668 [==============================] - 4044s 258ms/step - loss: 2.5662\n",
      "Epoch 30/30\n",
      "15668/15668 [==============================] - ETA: 0s - loss: 2.7819\n",
      "Epoch 00030: loss did not improve from 1.70483\n",
      "15668/15668 [==============================] - 4094s 261ms/step - loss: 2.7819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x224d7326e50>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model.fit(input_data, output_data, epochs = 30, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. the to the the sh the tee toe toe toe toe to the to the the th the the ao toe a sh the the tee the toe toe tee th tee to the ae the the the toe the toe aa t ae the tee to the toe the toe toe tne to toe the to the toe to the the toe to the toe an toe the to th th te the 1n th the toe th the toe the toe the soe the toe soe the  and toe th to t an te the te th th the the toe to th the the toe the toe tee toe th tne tee toe toe soe an the oh the toee the teee the tee toee to the th th tee tee toe toe toe toe to th tee toe ao the toe toe toe tee ane tee toe toe to th  ao th the te the toe toe the toe th the th th the the aed ae the toe toe toe toe toe soe toe toe toe an toe toe to the the soe toe the an the tee toe to  - the to the the the a to toe the the to th the the the toe soe to the the to th the to the th the aed to tee te the to the te to thee th th the to the the toe th the tee ao to the te the to th toe th the an the toe te the the the the the toe  ae th th the toe the to the je t\n"
     ]
    }
   ],
   "source": [
    "init = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "\n",
    "words = [char_dict[c] for c in init[-99:].lower()]\n",
    "\n",
    "characters = 0\n",
    "while characters <= 1000:\n",
    "    seq = np.reshape(words, (1, len(words), 1))\n",
    "    seq = seq / float(len(chars))\n",
    "    char_pred = LSTM_model.predict(seq, verbose = 0)\n",
    "    index = np.argmax(char_pred)\n",
    "    \n",
    "    if int_dict[index] != '\\n':\n",
    "        characters += 1\n",
    "        init += int_dict[index]\n",
    "    \n",
    "    words.append(index)\n",
    "    words = words[1:len(words)]\n",
    "\n",
    "print(init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
